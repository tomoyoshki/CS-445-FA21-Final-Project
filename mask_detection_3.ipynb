{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CS 445 Final Project\n",
    "\n",
    "## Real time tracking mask detection\n",
    "\n",
    "Tony Chang, Ruiyang Dai, Tomoyoshi Kimura"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torchvision.transforms import ToTensor, Scale, Compose\n",
    "\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from torchvision import transforms\n",
    "from torch.autograd import Variable\n",
    "\n",
    "from torch.utils.data import random_split\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.utils import make_grid\n",
    "\n",
    "import PIL\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import cv2\n",
    "\n",
    "import numpy as np\n",
    "from mtcnn import MTCNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image display function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_image(img, label):\n",
    "    print('Label: ', dataset.classes[label], \"(\"+str(label)+\")\")\n",
    "    plt.imshow(img.permute(1, 2, 0))  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8982\n",
      "['mask_weared_incorrect', 'with_mask', 'without_mask']\n"
     ]
    }
   ],
   "source": [
    "data_dir = './Dataset'\n",
    "dataset = ImageFolder(data_dir, transform=ToTensor())\n",
    "print(len(dataset))\n",
    "print(dataset.classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Image size and normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_size = (128,128)\n",
    "\n",
    "dataset = ImageFolder(data_dir, transforms.Compose([\n",
    "    # transforms.ColorJitter(0.1, 0.1, 0.1, 0.1),\n",
    "    transforms.Resize(image_size),\n",
    "    # transforms.RandomRotation(degrees=15),\n",
    "    # transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training validation partition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8 : 2 Train : Validation\n",
    "val_pct = 0.2\n",
    "val_size = int(val_pct * len(dataset))\n",
    "\n",
    "train_ds, valid_ds = random_split(dataset, [len(dataset) - val_size, val_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "\n",
    "train_dl = DataLoader(train_ds, batch_size, shuffle=True, num_workers=4, pin_memory=True)\n",
    "valid_dl = DataLoader(valid_ds, batch_size*2, num_workers=4, pin_memory=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Base class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(outputs, labels):\n",
    "    _, preds = torch.max(outputs, dim=1)\n",
    "    return torch.tensor(torch.sum(preds == labels).item() / len(preds))\n",
    "\n",
    "class ImageClassificationBase(nn.Module):\n",
    "    def training_step(self, batch):\n",
    "        images= batch[0].to(device)\n",
    "        labels = batch[1].to(device)\n",
    "        out = self(images)                  # Generate predictions\n",
    "        loss = F.cross_entropy(out, labels)  # Calculate loss\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch):\n",
    "        images = batch[0].to(device)\n",
    "        labels = batch[1].to(device)\n",
    "        out = self(images)                    # Generate predictions\n",
    "        loss = F.cross_entropy(out, labels)   # Calculate loss\n",
    "        acc = accuracy(out, labels)           # Calculate accuracy\n",
    "        return {'val_loss': loss.detach(), 'val_acc': acc}\n",
    "\n",
    "    def validation_epoch_end(self, outputs):\n",
    "        batch_losses = [x['val_loss'] for x in outputs]\n",
    "        epoch_loss = torch.stack(batch_losses).mean()   # Combine losses\n",
    "        batch_accs = [x['val_acc'] for x in outputs]\n",
    "        epoch_acc = torch.stack(batch_accs).mean()      # Combine accuracies\n",
    "        return {'val_loss': epoch_loss.item(), 'val_acc': epoch_acc.item()}\n",
    "\n",
    "    def epoch_end(self, epoch, result):\n",
    "        print(\"Epoch [{}],{} train_loss: {:.4f}, val_loss: {:.4f}, val_acc: {:.4f}\".format(\n",
    "            epoch, \"last_lr: {:.5f},\".format(result['lrs'][-1]) if 'lrs' in result else '', \n",
    "            result['train_loss'], result['val_loss'], result['val_acc']))\n",
    "\n",
    "    def softmax(self, x):\n",
    "        \"\"\"Compute softmax values for each sets of scores in x.\"\"\"\n",
    "        e_x = np.exp(x - np.max(x))\n",
    "        return e_x / e_x.sum()\n",
    "\n",
    "    def predict(self, image):\n",
    "        image = image.to(device)\n",
    "        out = self(image).cpu().data.numpy()[0]\n",
    "        softmax_out = self.softmax(out)\n",
    "        res = np.argmax(softmax_out)\n",
    "        return res, softmax_out\n",
    "\n",
    "    def save_model(self, path):\n",
    "        torch.save(self, path)\n",
    "    \n",
    "\n",
    "    def load_model(self, path):\n",
    "        self.load_state_dict(torch.load(path))\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def evaluate(model, val_loader):\n",
    "    model.eval()\n",
    "    outputs = [model.validation_step(batch) for batch in val_loader]\n",
    "    return model.validation_epoch_end(outputs)\n",
    "\n",
    "def fit(epochs, lr, model, train_loader, val_loader, opt_func=torch.optim.SGD):\n",
    "    history = []\n",
    "    optimizer = opt_func(model.parameters(), lr)\n",
    "    for epoch in range(epochs):\n",
    "        # Training Phase\n",
    "        model.train()\n",
    "        train_losses = []\n",
    "        for batch in tqdm(train_loader):\n",
    "            loss = model.training_step(batch)\n",
    "            train_losses.append(loss)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "        # Validation phase\n",
    "        result = evaluate(model, val_loader)\n",
    "        result['train_loss'] = torch.stack(train_losses).mean().item()\n",
    "        model.epoch_end(epoch, result)\n",
    "        history.append(result)\n",
    "    return history\n",
    "\n",
    "def get_lr(optimizer):\n",
    "    for param_group in optimizer.param_groups:\n",
    "        return param_group['lr']\n",
    "\n",
    "def fit_one_cycle(epochs, max_lr, model, train_loader, val_loader,\n",
    "                  weight_decay=0, grad_clip=None, opt_func=torch.optim.SGD):\n",
    "    torch.cuda.empty_cache()\n",
    "    history = []\n",
    "\n",
    "    # Set up custom optimizer with weight decay\n",
    "    optimizer = opt_func(model.parameters(), max_lr, weight_decay=weight_decay)\n",
    "    # Set up one-cycle learning rate scheduler\n",
    "    sched = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr, epochs=epochs,\n",
    "                                                steps_per_epoch=len(train_loader))\n",
    "\n",
    "    print(\"Start training %d epochs\" % (epochs))\n",
    "    for epoch in range(epochs):\n",
    "        # Training Phase\n",
    "        model.train().to(device)\n",
    "        train_losses = []\n",
    "        lrs = []\n",
    "        for batch in tqdm(train_loader):\n",
    "            loss = model.training_step(batch)\n",
    "            train_losses.append(loss)\n",
    "            loss.backward()\n",
    "\n",
    "            # Gradient clipping\n",
    "            if grad_clip:\n",
    "                nn.utils.clip_grad_value_(model.parameters(), grad_clip)\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Record & update learning rate\n",
    "            lrs.append(get_lr(optimizer))\n",
    "            sched.step()\n",
    "        # Validation phase\n",
    "        result = evaluate(model, val_loader)\n",
    "        result['train_loss'] = torch.stack(train_losses).mean().item()\n",
    "        result['lrs'] = lrs\n",
    "        model.epoch_end(epoch, result)\n",
    "        history.append(result)\n",
    "    return history\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data loader for prediction image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ktomo\\Documents\\CS 445\\venv\\lib\\site-packages\\torchvision\\transforms\\transforms.py:317: UserWarning: The use of the transforms.Scale transform is deprecated, please use transforms.Resize instead.\n",
      "  warnings.warn(\"The use of the transforms.Scale transform is deprecated, \" +\n"
     ]
    }
   ],
   "source": [
    "loader = Compose([Scale(image_size), ToTensor()])\n",
    "\n",
    "\n",
    "def image_loader(image_name):\n",
    "    \"\"\"load image, returns cuda tensor\"\"\"\n",
    "    image = PIL.Image.open(image_name)\n",
    "    image = loader(image).float()\n",
    "    image = Variable(image, requires_grad=True)\n",
    "    image = image.unsqueeze(0)  #this is for VGG, may not be needed for ResNet\n",
    "    return image.to(device)  #assumes that you're using GPU\n",
    "\n",
    "def prepare_image(image):\n",
    "    image = loader(image).float()\n",
    "    image = Variable(image, requires_grad=True)\n",
    "    image = image.unsqueeze(0)  #this is for VGG, may not be needed for ResNet\n",
    "    return image.to(device)  #assumes that you're using GPU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convolution Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transfer Learning with ResNet 34"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import models\n",
    "models.resnet34(pretrained=True)\n",
    "\n",
    "class MasksModel(ImageClassificationBase):\n",
    "\n",
    "    def __init__(self, num_classes, pretrained=True):\n",
    "        super().__init__()\n",
    "        # Use a pretrained model\n",
    "        self.network = models.resnet34(pretrained=pretrained) #downloading weights from this model when it was trained on ImageNet dataset\n",
    "        # Replace last layer\n",
    "        self.network.fc = nn.Linear(self.network.fc.in_features, num_classes)\n",
    "\n",
    "    def forward(self, xb):\n",
    "        return self.network(xb)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training 20 epochs\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5d8dbc774fbb4659a76812fb6667a2f4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/113 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [0],last_lr: 0.00010, train_loss: 0.1249, val_loss: 0.0100, val_acc: 0.9979\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b17edb6f5bb43d1b5bb50f3730fc63d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/113 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1],last_lr: 0.00028, train_loss: 0.0175, val_loss: 0.0305, val_acc: 0.9922\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e1e6579f3f6749c2a8cfa2feff640b71",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/113 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2],last_lr: 0.00052, train_loss: 0.0507, val_loss: 0.0506, val_acc: 0.9839\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2e429e48d96b4a5a9c8e682a8d2f7c4f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/113 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3],last_lr: 0.00076, train_loss: 0.0396, val_loss: 0.0608, val_acc: 0.9833\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f107709629649db9753daf25f69a482",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/113 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4],last_lr: 0.00094, train_loss: 0.0464, val_loss: 0.0297, val_acc: 0.9911\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b8e77733c28f45cc98c20a7c91c65594",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/113 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5],last_lr: 0.00100, train_loss: 0.0329, val_loss: 0.0709, val_acc: 0.9844\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d19d5fc81e10420e89023bc10ac15c48",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/113 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [6],last_lr: 0.00099, train_loss: 0.0250, val_loss: 0.0347, val_acc: 0.9922\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "56e2e1487de44d3a984aa321ac8d4ae8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/113 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [7],last_lr: 0.00095, train_loss: 0.0086, val_loss: 0.0250, val_acc: 0.9932\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6697e7147d0946e187fe55bdd12acacb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/113 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [8],last_lr: 0.00089, train_loss: 0.0128, val_loss: 0.0204, val_acc: 0.9948\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3057b63161f34dc3b5aeaf2d803a02c1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/113 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [9],last_lr: 0.00081, train_loss: 0.0106, val_loss: 0.0287, val_acc: 0.9927\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f2d3211713d247bb95c17cf79e48c194",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/113 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10],last_lr: 0.00072, train_loss: 0.0041, val_loss: 0.0166, val_acc: 0.9943\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c3e50f0f9e043a8a9ed777c4b5ea200",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/113 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [11],last_lr: 0.00061, train_loss: 0.0002, val_loss: 0.0215, val_acc: 0.9953\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "75572fa79e224ffb9dea7b9647978ba4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/113 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [12],last_lr: 0.00050, train_loss: 0.0002, val_loss: 0.0207, val_acc: 0.9958\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e2deeb8590e74467a06ef2eb67ece5ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/113 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [13],last_lr: 0.00039, train_loss: 0.0001, val_loss: 0.0206, val_acc: 0.9958\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0e88e5ddbd914e5f89760bac0c436008",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/113 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [14],last_lr: 0.00028, train_loss: 0.0000, val_loss: 0.0215, val_acc: 0.9958\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e754e08e733743a2a1c16e430914ed30",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/113 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [15],last_lr: 0.00019, train_loss: 0.0000, val_loss: 0.0222, val_acc: 0.9958\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b91997ce00744de485764be98f60fa83",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/113 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [16],last_lr: 0.00011, train_loss: 0.0000, val_loss: 0.0197, val_acc: 0.9953\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9acf363bb2ad43e49f7725c879b822ae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/113 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [17],last_lr: 0.00005, train_loss: 0.0000, val_loss: 0.0247, val_acc: 0.9958\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e088c19e7f64c9db89abca07b477269",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/113 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [18],last_lr: 0.00001, train_loss: 0.0000, val_loss: 0.0265, val_acc: 0.9953\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d797df62728142f2a4a66c798b8c28fc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/113 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [19],last_lr: 0.00000, train_loss: 0.0000, val_loss: 0.0218, val_acc: 0.9958\n"
     ]
    }
   ],
   "source": [
    "model = MasksModel(len(dataset.classes), pretrained=True).to(device)\n",
    "\n",
    "history = fit_one_cycle(20, 0.001, model, train_dl, valid_dl, opt_func=torch.optim.Adam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_model(\"./saved_model/m1.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Realtime Tracking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect(image):\n",
    "\timage_gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "\tface_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + \"haarcascade_frontalface_default.xml\")\n",
    "\t# 检测图像中的所有人脸\n",
    "\tfaces = face_cascade.detectMultiScale(image_gray, minNeighbors=3)\n",
    "\t# print(f\"{len(faces)} faces detected in the image.\")\n",
    "\treturn faces\n",
    "\n",
    "font = cv2.FONT_HERSHEY_SIMPLEX  \n",
    "# fontScale\n",
    "fontScale = 0.8\n",
    "# Line thickness of 2 px\n",
    "thickness = 1\n",
    "\n",
    "red = (0, 0, 255)\n",
    "blue = (255, 0, 0)\n",
    "green = (0, 255, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Track"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv2.namedWindow(\"preview\")\n",
    "vc = cv2.VideoCapture(0)\n",
    "\n",
    "if vc.isOpened(): # try to get the first frame\n",
    "    rval, frame = vc.read()\n",
    "else:\n",
    "    rval = False\n",
    "\n",
    "count = -1\n",
    "pf = 6\n",
    "thres = 0.8\n",
    "pos = []\n",
    "\n",
    "while rval:\n",
    "    \n",
    "    faces = detect(frame)\n",
    "\n",
    "    count = (count + 1) % pf\n",
    "    \n",
    "    if count == 0:\n",
    "        pos = []\n",
    "        for x, y, width, height in faces:\n",
    "            # get face\n",
    "            face = cv2.cvtColor(frame[y:y+height, x:x+width], cv2.COLOR_BGR2RGB)\n",
    "            # prepare image\n",
    "            im = PIL.Image.fromarray(face)\n",
    "            # predict\n",
    "            result, output = model.predict(prepare_image(im))\n",
    "            \n",
    "            pos.append([x, y, width, height, result, output])\n",
    "            # No mask\n",
    "            # if result == 2 and output[2] > thres:\n",
    "            #     # print(\"No mask worn\")\n",
    "            #     cv2.rectangle(frame, (x, y), (x + width, y + height), color=(0, 0, 255), thickness=2)\n",
    "            #     cv2.putText(frame, \"Negative %f\" % output[2], (x + int(width*0.05),  y + int(height*0.1)), font, 0.5, red, thickness, cv2.LINE_AA)\n",
    "            # # has mask\n",
    "            # elif result == 1 and output[1] > thres:\n",
    "            #     cv2.rectangle(frame, (x, y), (x + width, y + height), color=(0, 255, 0), thickness=2)\n",
    "            #     cv2.putText(frame, \"Positive %f\" % output[1], (x + int(width*0.05),  y + int(height*0.1)), font, 0.5, green, thickness, cv2.LINE_AA)\n",
    "            # # incorrectly worn\n",
    "            # else:\n",
    "            #     cv2.rectangle(frame, (x, y), (x + width, y + height), color=(255, 0, 0), thickness=2)\n",
    "            #     cv2.putText(frame, \"Incorrectly %f\" % output[0], (x + int(width*0.05),  y + int(height*0.1)), font, 0.5, blue, thickness, cv2.LINE_AA)\n",
    "    for x, y, width, height, res, out in pos:\n",
    "            # No mask\n",
    "            if result == 2 and output[2] > thres:\n",
    "                # print(\"No mask worn\")\n",
    "                cv2.rectangle(frame, (x, y), (x + width, y + height), color=(0, 0, 255), thickness=2)\n",
    "                cv2.putText(frame, \"Negative %f\" % output[2], (x + int(width*0.05),  y + int(height*0.1)), font, 0.5, red, thickness, cv2.LINE_AA)\n",
    "            # has mask\n",
    "            elif result == 1 and output[1] > thres:\n",
    "                cv2.rectangle(frame, (x, y), (x + width, y + height), color=(0, 255, 0), thickness=2)\n",
    "                cv2.putText(frame, \"Positive %f\" % output[1], (x + int(width*0.05),  y + int(height*0.1)), font, 0.5, green, thickness, cv2.LINE_AA)\n",
    "            # incorrectly worn\n",
    "            else:\n",
    "                cv2.rectangle(frame, (x, y), (x + width, y + height), color=(255, 0, 0), thickness=2)\n",
    "                cv2.putText(frame, \"Incorrectly %f\" % output[0], (x + int(width*0.05),  y + int(height*0.1)), font, 0.5, blue, thickness, cv2.LINE_AA)\n",
    "    cv2.imshow(\"preview\", frame)\n",
    "\n",
    "    # Read next frame\n",
    "    rval, frame = vc.read()\n",
    "\n",
    "    # Exit\n",
    "    key = cv2.waitKey(20)\n",
    "    if key == 27: # exit on ESC\n",
    "        break\n",
    "\n",
    "vc.release()\n",
    "cv2.destroyWindow(\"preview\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "8b9d182a394eb62327ff08c09521c268cb2f0b110a56a7a3eb00ea904bf915f1"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit ('base': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
